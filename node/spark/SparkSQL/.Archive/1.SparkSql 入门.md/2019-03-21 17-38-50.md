# SparkSql 入门

## 起点：SparkSession

Spark中所有功能的入口点都是SparkSession类。要创建基本的SparkSession，只需使用`SparkSession.builder()`：

``` scala
import org.apache.spark.sql.SparkSession

val spark = SparkSession
  .builder()
  .appName("Spark SQL basic example")
  .config("spark.some.config.option", "some-value")
  .getOrCreate()

import spark.implicits._
```

## 创建和使用DataFrame

DataFrame是无类型数据集，可以通过多种方式创建

``` scala
val df = spark.read.json("examples/src/main/resources/people.json")
df.show()

//打印Schema
df.printSchema()

//查询指定列
df.select("name").show()

// 查询两个字段，并将age+1
df.select($"name", $"age" + 1).show()

//条件查询
df.filter($"age" > 21).show()

//分组统计
df.groupBy("age").count().show()

//也可以直接使用sql语句查询
val sqlDF = spark.sql("SELECT * FROM people")
```

## 全局临时视图

Temporary View（临时表）：为一个DataFrame数据创建一个临时的表名，方便进行SQL查询。**临时视图的生命周期是Session范围的**，也就是当Session销毁时，临时表也就销毁了。

Global Temporary View（全局临时表）：是所有Session之间共享的临时视图，它与系统保留的数据库进行绑定`global_temp`。使用时必须使用限定名进行引用。

``` scala
//将DataFrame注册为全局临时视图
df.createGlobalTempView("people")
//全局临时视图绑定到系统保留的数据库 global_temp
spark.sql("SELECT * FROM global_temp.people").show()

//全局临时视图是跨会话
spark.newSession().sql("SELECT * FROM global_temp.people").show()
```

## 创建和使用DataSet

DataSet和RDD类似，但是它不使用Java序列化或Kryo，而是使用专用的编码器来序列化对象以便通过网络进行处理或传输。虽然编码器和标准序列化都负责将对象转换为字节，但编码器是动态生成的代码，并使用一种格式，允许Spark执行许多操作，如过滤，排序和散列，而**无需将字节反序列化为对象**。

``` scala
case class Person(name: String, age: Long)
// 为样例类创建解码器
val caseClassDS = Seq(Person("Andy", 32)).toDS()
caseClassDS.show()

// 最常见的编解码器由 importing spark.implicits._ 自动提供
val primitiveDS = Seq(1, 2, 3).toDS()
primitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4)

// 可以直接通过类名将数据转化为DataSet
val path = "examples/src/main/resources/people.json"
val peopleDS = spark.read.json(path).as[Person]
peopleDS.show()
```
## RDD的互操作性

Spark SQL支持两种不同的方法将已存在的RDD转换为DataSet

- 使用反射来推断包含特定类型对象的RDD的模式。这种基于反射的方法可以提供更简洁的代码，当然只能是编程的时候就已经知道了数据的结构。
- 通过编程接口，构建Schema应用于现有RDD。此方法适用于事先不知道其数据结构的情况。

### 使用反射推断Schema
Spark SQL的Scala接口支持自动转换一个包含样例类的RDD为`DataFrame.Case class`定义了表的 `Schema.Case class` 的参数名使用反射读取并且成为了列名.Case class 也可以是嵌套的或者包含像 Seq 或者 Array 这样的复杂类型.这个RDD能够**被隐式转换成一个DataFrame然后被注册为一个表**.表可以用于后续的 SQL 语句。
```scala
import spark.implicits._
//从文本中创建RDD,在转换为DataFrame
val peopleDF = spark.sparkContext
  .textFile("examples/src/main/resources/people.txt")
  .map(_.split(","))
  .map(attributes => Person(attributes(0), attributes(1).trim.toInt))
  .toDF()
peopleDF.createOrReplaceTempView("people")
//可以使用sql语句进行查询
val teenagersDF = spark.sql("SELECT name, age FROM people WHERE age BETWEEN 13 AND 19")

//可以直接使用字段的索引访问数据
teenagersDF.map(teenager => "Name: " + teenager(0)).show()
//使用字段名访问数据
teenagersDF.map(teenager => "Name: " + teenager.getAs[String]("name")).show()
// +------------+
// |       value|
// +------------+
// |Name: Justin|
// +------------+

// 因为Dataset[Map[K,V]]没有明确的编码器, 需要定义一个编码器
implicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]]
// 原来的样例类也可以定义为:
// implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()

// row.getValuesMap[T] 遍历列。
teenagersDF.map(teenager => teenager.getValuesMap[Any](List("name", "age"))).collect()
// Array(Map("name" -> "Justin", "age" -> 19))
```
### 以编程的方式指定Schema
当`case class`不能够在执行之前被定义（例如, records记录的结构在一个string字符串中被编码了, 或者一个 text 文本 dataset 将被解析并且不同的用户投影的字段是不一样的）.一个DataFrame可以使用下面的三步以编程的方式来创建.
1. 从原始的RDD创建RDD的Row（行）;
2. 创建 Schema 表示一个 StructType 匹配 RDD 中的 Row（行）的结构.
3. 通过 SparkSession 提供的 createDataFrame 方法应用 Schema 到 RDD 的 RowS（行）.

```scala
import org.apache.spark.sql.types._

val peopleRDD = spark.sparkContext.textFile("examples/src/main/resources/people.txt")
val schemaString = "name age"

// 通过字符串创建schema
val fields = schemaString.split(" ")
  .map(fieldName => StructField(fieldName, StringType, nullable = true))
val schema = StructType(fields)

// 处理RDD，按字段分割
val rowRDD = peopleRDD
  .map(_.split(","))
  .map(attributes => Row(attributes(0), attributes(1).trim))

//将Schema与RDD进行匹配
val peopleDF = spark.createDataFrame(rowRDD, schema)

peopleDF.createOrReplaceTempView("people")
val results = spark.sql("SELECT name FROM people")

results.map(attributes => "Name: " + attributes(0)).show()
// +-------------+
// |        value|
// +-------------+
// |Name: Michael|
// |   Name: Andy|
// | Name: Justin|
// +-------------+
```
## 聚合函数
DataFrame内置有聚合函数，例如:`count()`，`countDistinct()`，`avg()`，`max()`，`min()`等，除此之外SparkSQL还提供有类型的聚合函数，用于也可以自定义这两种聚合函数。

### 无类型用户自定义聚合函数
用户必须基础UserDefinedAggregateFunction 抽象类以实现自定义无类型聚合函数
### 类型安全的用户自定义聚合函数

