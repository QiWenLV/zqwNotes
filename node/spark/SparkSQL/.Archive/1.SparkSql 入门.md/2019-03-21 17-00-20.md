# SparkSql 入门

## 起点：SparkSession

Spark中所有功能的入口点都是SparkSession类。要创建基本的SparkSession，只需使用`SparkSession.builder()`：

``` scala
import org.apache.spark.sql.SparkSession

val spark = SparkSession
  .builder()
  .appName("Spark SQL basic example")
  .config("spark.some.config.option", "some-value")
  .getOrCreate()

import spark.implicits._
```

## 创建和使用DataFrame

DataFrame是无类型数据集，可以通过多种方式创建

``` scala
val df = spark.read.json("examples/src/main/resources/people.json")
df.show()

//打印Schema
df.printSchema()

//查询指定列
df.select("name").show()

// 查询两个字段，并将age+1
df.select($"name", $"age" + 1).show()

//条件查询
df.filter($"age" > 21).show()

//分组统计
df.groupBy("age").count().show()

//也可以直接使用sql语句查询
val sqlDF = spark.sql("SELECT * FROM people")
```

## 全局临时视图

Temporary View（临时表）：为一个DataFrame数据创建一个临时的表名，方便进行SQL查询。**临时视图的生命周期是Session范围的**，也就是当Session销毁时，临时表也就销毁了。

Global Temporary View（全局临时表）：是所有Session之间共享的临时视图，它与系统保留的数据库进行绑定`global_temp`。使用时必须使用限定名进行引用。

``` scala
//将DataFrame注册为全局临时视图
df.createGlobalTempView("people")
//全局临时视图绑定到系统保留的数据库 global_temp
spark.sql("SELECT * FROM global_temp.people").show()

//全局临时视图是跨会话
spark.newSession().sql("SELECT * FROM global_temp.people").show()
```

## 创建和使用DataSet

DataSet和RDD类似，但是它不使用Java序列化或Kryo，而是使用专用的编码器来序列化对象以便通过网络进行处理或传输。虽然编码器和标准序列化都负责将对象转换为字节，但编码器是动态生成的代码，并使用一种格式，允许Spark执行许多操作，如过滤，排序和散列，而**无需将字节反序列化为对象**。

``` scala
case class Person(name: String, age: Long)
// 为样例类创建解码器
val caseClassDS = Seq(Person("Andy", 32)).toDS()
caseClassDS.show()

// 最常见的编解码器由 importing spark.implicits._ 自动提供
val primitiveDS = Seq(1, 2, 3).toDS()
primitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4)

// 可以直接通过类名将数据转化为DataSet
val path = "examples/src/main/resources/people.json"
val peopleDS = spark.read.json(path).as[Person]
peopleDS.show()
```
Spark SQL支持两种不同的方法将现有RDD转换为数据集：

- 使用反射来推断包含特定类型对象的RDD的模式。这种基于反射的方法可以提供更简洁的代码，当然只能是编程的时候就已经知道了数据的结构。
    ```scala
    import spark.implicits._
    //从文本中创建RDD,在转换为DataFrame
    val peopleDF = spark.sparkContext
      .textFile("examples/src/main/resources/people.txt")
      .map(_.split(","))
      .map(attributes => Person(attributes(0), attributes(1).trim.toInt))
      .toDF()
    peopleDF.createOrReplaceTempView("people")
    //可以使用sql语句进行查询
    val teenagersDF = spark.sql("SELECT name, age FROM people WHERE age BETWEEN 13 AND 19")
    
    //可以直接使用字段的索引访问数据
    teenagersDF.map(teenager => "Name: " + teenager(0)).show()
    //使用字段名访问数据
    teenagersDF.map(teenager => "Name: " + teenager.getAs[String]("name")).show()
    // +------------+
    // |       value|
    // +------------+
    // |Name: Justin|
    // +------------+
    
    // No pre-defined encoders for Dataset[Map[K,V]], define explicitly
    implicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]]
    // Primitive types and case classes can be also defined as
    // implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()
    
    // row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]
    teenagersDF.map(teenager => teenager.getValuesMap[Any](List("name", "age"))).collect()
    // Array(Map("name" -> "Justin", "age" -> 19))
    ```

- 通过编程接口，构建Schema应用于现有RDD。此方法适用于事先不知道其数据结构的情况。
