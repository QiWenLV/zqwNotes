# SparkSql 入门

## 起点：SparkSession

Spark中所有功能的入口点都是SparkSession类。要创建基本的SparkSession，只需使用`SparkSession.builder()`：

``` scala
import org.apache.spark.sql.SparkSession

val spark = SparkSession
  .builder()
  .appName("Spark SQL basic example")
  .config("spark.some.config.option", "some-value")
  .getOrCreate()

import spark.implicits._
```

## 创建和使用DataFrame

DataFrame是无类型数据集，可以通过多种方式创建

``` scala
val df = spark.read.json("examples/src/main/resources/people.json")
df.show()

//打印Schema
df.printSchema()

//查询指定列
df.select("name").show()

// 查询两个字段，并将age+1
df.select($"name", $"age" + 1).show()

//条件查询
df.filter($"age" > 21).show()

//分组统计
df.groupBy("age").count().show()

//也可以直接使用sql语句查询
val sqlDF = spark.sql("SELECT * FROM people")
```

## 全局临时视图

Temporary View（临时表）：为一个DataFrame数据创建一个临时的表名，方便进行SQL查询。**临时视图的生命周期是Session范围的**，也就是当Session销毁时，临时表也就销毁了。

Global Temporary View（全局临时表）：是所有Session之间共享的临时视图，它与系统保留的数据库进行绑定`global_temp`。使用时必须使用限定名进行引用。

``` scala
//将DataFrame注册为全局临时视图
df.createGlobalTempView("people")
//全局临时视图绑定到系统保留的数据库 global_temp
spark.sql("SELECT * FROM global_temp.people").show()

//全局临时视图是跨会话
spark.newSession().sql("SELECT * FROM global_temp.people").show()
```

## 创建和使用DataSet

DataSet和RDD类似，但是它不使用Java序列化或Kryo，而是使用专用的编码器来序列化对象以便通过网络进行处理或传输。虽然编码器和标准序列化都负责将对象转换为字节，但编码器是动态生成的代码，并使用一种格式，允许Spark执行许多操作，如过滤，排序和散列，而**无需将字节反序列化为对象**。

``` scala
case class Person(name: String, age: Long)

// Encoders are created for case classes
val caseClassDS = Seq(Person("Andy", 32)).toDS()
caseClassDS.show()
// +----+---+
// |name|age|
// +----+---+
// |Andy| 32|
// +----+---+

// Encoders for most common types are automatically provided by importing spark.implicits._
val primitiveDS = Seq(1, 2, 3).toDS()
primitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4)

// DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name
val path = "examples/src/main/resources/people.json"
val peopleDS = spark.read.json(path).as[Person]
peopleDS.show()
```

