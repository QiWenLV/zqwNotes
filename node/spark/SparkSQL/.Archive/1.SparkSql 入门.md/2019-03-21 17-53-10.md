# SparkSql 入门

## 起点：SparkSession

Spark中所有功能的入口点都是SparkSession类。要创建基本的SparkSession，只需使用`SparkSession.builder()`：

``` scala
import org.apache.spark.sql.SparkSession

val spark = SparkSession
  .builder()
  .appName("Spark SQL basic example")
  .config("spark.some.config.option", "some-value")
  .getOrCreate()

import spark.implicits._
```

## 创建和使用DataFrame

DataFrame是无类型数据集，可以通过多种方式创建

``` scala
val df = spark.read.json("examples/src/main/resources/people.json")
df.show()

//打印Schema
df.printSchema()

//查询指定列
df.select("name").show()

// 查询两个字段，并将age+1
df.select($"name", $"age" + 1).show()

//条件查询
df.filter($"age" > 21).show()

//分组统计
df.groupBy("age").count().show()

//也可以直接使用sql语句查询
val sqlDF = spark.sql("SELECT * FROM people")
```

## 全局临时视图

Temporary View（临时表）：为一个DataFrame数据创建一个临时的表名，方便进行SQL查询。**临时视图的生命周期是Session范围的**，也就是当Session销毁时，临时表也就销毁了。

Global Temporary View（全局临时表）：是所有Session之间共享的临时视图，它与系统保留的数据库进行绑定`global_temp`。使用时必须使用限定名进行引用。

``` scala
//将DataFrame注册为全局临时视图
df.createGlobalTempView("people")
//全局临时视图绑定到系统保留的数据库 global_temp
spark.sql("SELECT * FROM global_temp.people").show()

//全局临时视图是跨会话
spark.newSession().sql("SELECT * FROM global_temp.people").show()
```

## 创建和使用DataSet

DataSet和RDD类似，但是它不使用Java序列化或Kryo，而是使用专用的编码器来序列化对象以便通过网络进行处理或传输。虽然编码器和标准序列化都负责将对象转换为字节，但编码器是动态生成的代码，并使用一种格式，允许Spark执行许多操作，如过滤，排序和散列，而**无需将字节反序列化为对象**。

``` scala
case class Person(name: String, age: Long)
// 为样例类创建解码器
val caseClassDS = Seq(Person("Andy", 32)).toDS()
caseClassDS.show()

// 最常见的编解码器由 importing spark.implicits._ 自动提供
val primitiveDS = Seq(1, 2, 3).toDS()
primitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4)

// 可以直接通过类名将数据转化为DataSet
val path = "examples/src/main/resources/people.json"
val peopleDS = spark.read.json(path).as[Person]
peopleDS.show()
```
## RDD的互操作性

Spark SQL支持两种不同的方法将已存在的RDD转换为DataSet

- 使用反射来推断包含特定类型对象的RDD的模式。这种基于反射的方法可以提供更简洁的代码，当然只能是编程的时候就已经知道了数据的结构。
- 通过编程接口，构建Schema应用于现有RDD。此方法适用于事先不知道其数据结构的情况。

### 使用反射推断Schema
Spark SQL的Scala接口支持自动转换一个包含样例类的RDD为`DataFrame.Case class`定义了表的 `Schema.Case class` 的参数名使用反射读取并且成为了列名.Case class 也可以是嵌套的或者包含像 Seq 或者 Array 这样的复杂类型.这个RDD能够**被隐式转换成一个DataFrame然后被注册为一个表**.表可以用于后续的 SQL 语句。
```scala
import spark.implicits._
//从文本中创建RDD,在转换为DataFrame
val peopleDF = spark.sparkContext
  .textFile("examples/src/main/resources/people.txt")
  .map(_.split(","))
  .map(attributes => Person(attributes(0), attributes(1).trim.toInt))
  .toDF()
peopleDF.createOrReplaceTempView("people")
//可以使用sql语句进行查询
val teenagersDF = spark.sql("SELECT name, age FROM people WHERE age BETWEEN 13 AND 19")

//可以直接使用字段的索引访问数据
teenagersDF.map(teenager => "Name: " + teenager(0)).show()
//使用字段名访问数据
teenagersDF.map(teenager => "Name: " + teenager.getAs[String]("name")).show()
// +------------+
// |       value|
// +------------+
// |Name: Justin|
// +------------+

// 因为Dataset[Map[K,V]]没有明确的编码器, 需要定义一个编码器
implicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]]
// 原来的样例类也可以定义为:
// implicit val stringIntMapEncoder: Encoder[Map[String, Any]] = ExpressionEncoder()

// row.getValuesMap[T] 遍历列。
teenagersDF.map(teenager => teenager.getValuesMap[Any](List("name", "age"))).collect()
// Array(Map("name" -> "Justin", "age" -> 19))
```
### 以编程的方式指定Schema
当`case class`不能够在执行之前被定义（例如, records记录的结构在一个string字符串中被编码了, 或者一个 text 文本 dataset 将被解析并且不同的用户投影的字段是不一样的）.一个DataFrame可以使用下面的三步以编程的方式来创建.
1. 从原始的RDD创建RDD的Row（行）;
2. 创建 Schema 表示一个 StructType 匹配 RDD 中的 Row（行）的结构.
3. 通过 SparkSession 提供的 createDataFrame 方法应用 Schema 到 RDD 的 RowS（行）.

```scala
import org.apache.spark.sql.types._

val peopleRDD = spark.sparkContext.textFile("examples/src/main/resources/people.txt")
val schemaString = "name age"

// 通过字符串创建schema
val fields = schemaString.split(" ")
  .map(fieldName => StructField(fieldName, StringType, nullable = true))
val schema = StructType(fields)

// 处理RDD，按字段分割
val rowRDD = peopleRDD
  .map(_.split(","))
  .map(attributes => Row(attributes(0), attributes(1).trim))

//将Schema与RDD进行匹配
val peopleDF = spark.createDataFrame(rowRDD, schema)

peopleDF.createOrReplaceTempView("people")
val results = spark.sql("SELECT name FROM people")

results.map(attributes => "Name: " + attributes(0)).show()
// +-------------+
// |        value|
// +-------------+
// |Name: Michael|
// |   Name: Andy|
// | Name: Justin|
// +-------------+
```
## 聚合函数
DataFrame内置有聚合函数，例如:`count()`，`countDistinct()`，`avg()`，`max()`，`min()`等，除此之外SparkSQL还提供有类型的聚合函数，用于也可以自定义这两种聚合函数。

### 无类型用户自定义聚合函数
用户必须继承 `UserDefinedAggregateFunction` 抽象类以实现自定义无类型聚合函数

```scala
object MyAverage extends UserDefinedAggregateFunction {
  // Data types of input arguments of this aggregate function
  def inputSchema: StructType = StructType(StructField("inputColumn", LongType) :: Nil)
  // Data types of values in the aggregation buffer
  def bufferSchema: StructType = {
    StructType(StructField("sum", LongType) :: StructField("count", LongType) :: Nil)
  }
  // The data type of the returned value
  def dataType: DataType = DoubleType
  // Whether this function always returns the same output on the identical input
  def deterministic: Boolean = true
  // Initializes the given aggregation buffer. The buffer itself is a `Row` that in addition to
  // standard methods like retrieving a value at an index (e.g., get(), getBoolean()), provides
  // the opportunity to update its values. Note that arrays and maps inside the buffer are still
  // immutable.
  def initialize(buffer: MutableAggregationBuffer): Unit = {
    buffer(0) = 0L
    buffer(1) = 0L
  }
  // Updates the given aggregation buffer `buffer` with new input data from `input`
  def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
    if (!input.isNullAt(0)) {
      buffer(0) = buffer.getLong(0) + input.getLong(0)
      buffer(1) = buffer.getLong(1) + 1
    }
  }
  // Merges two aggregation buffers and stores the updated buffer values back to `buffer1`
  def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
    buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0)
    buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1)
  }
  // Calculates the final result
  def evaluate(buffer: Row): Double = buffer.getLong(0).toDouble / buffer.getLong(1)
}

// Register the function to access it
spark.udf.register("myAverage", MyAverage)

val df = spark.read.json("examples/src/main/resources/employees.json")
df.createOrReplaceTempView("employees")
df.show()
// +-------+------+
// |   name|salary|
// +-------+------+
// |Michael|  3000|
// |   Andy|  4500|
// | Justin|  3500|
// |  Berta|  4000|
// +-------+------+

val result = spark.sql("SELECT myAverage(salary) as average_salary FROM employees")
result.show()
// +--------------+
// |average_salary|
// +--------------+
// |        3750.0|
// +--------------+
```
### 类型安全的用户自定义聚合函数
强类型数据集的用户定义聚合围绕Aggregator抽象类。

```scala
case class Employee(name: String, salary: Long)
case class Average(var sum: Long, var count: Long)

object MyAverage extends Aggregator[Employee, Average, Double] {
  // A zero value for this aggregation. Should satisfy the property that any b + zero = b
  def zero: Average = Average(0L, 0L)
  // Combine two values to produce a new value. For performance, the function may modify `buffer`
  // and return it instead of constructing a new object
  def reduce(buffer: Average, employee: Employee): Average = {
    buffer.sum += employee.salary
    buffer.count += 1
    buffer
  }
  // Merge two intermediate values
  def merge(b1: Average, b2: Average): Average = {
    b1.sum += b2.sum
    b1.count += b2.count
    b1
  }
  // Transform the output of the reduction
  def finish(reduction: Average): Double = reduction.sum.toDouble / reduction.count
  // Specifies the Encoder for the intermediate value type
  def bufferEncoder: Encoder[Average] = Encoders.product
  // Specifies the Encoder for the final output value type
  def outputEncoder: Encoder[Double] = Encoders.scalaDouble
}

val ds = spark.read.json("examples/src/main/resources/employees.json").as[Employee]
ds.show()
// +-------+------+
// |   name|salary|
// +-------+------+
// |Michael|  3000|
// |   Andy|  4500|
// | Justin|  3500|
// |  Berta|  4000|
// +-------+------+

// Convert the function to a `TypedColumn` and give it a name
val averageSalary = MyAverage.toColumn.name("average_salary")
val result = ds.select(averageSalary)
result.show()
// +--------------+
// |average_salary|
// +--------------+
// |        3750.0|
// +--------------+
```
