title: 2.SparkSQL 数据源
date:2019-03-21 17:44:53

[TOC]

# SparkSQL数据源

## 通用加载

默认加载parquet文件
```scala
val usersDF = spark.read.load("examples/src/main/resources/users.parquet")
usersDF.select("name", "favorite_color").write.save("namesAndFavColors.parquet")

//加载json文件
val peopleDF = spark.read.format("json").load("examples/src/main/resources/people.json")
peopleDF.select("name", "age").write.format("parquet").save("namesAndAges.parquet")

//加载csv文件，手动指定参数
val peopleDFCsv = spark.read.format("csv")
  .option("sep", ";")
  .option("inferSchema", "true")
  .option("header", "true")
  .load("examples/src/main/resources/people.csv")
  
//将数据写出为orc格式，需要设置bloom filters和dictionary encodings
usersDF.write.format("orc")
  .option("orc.bloom.filter.columns", "favorite_color")
  .option("orc.dictionary.key.threshold", "1.0")
  .save("users_with_options.orc")
```
### 直接对文件进行SQL查询

可以直接使用SQL查询该文件，而不是使用读取API将文件加载到DataFrame并进行查询。
```scala
val sqlDF = spark.sql("SELECT * FROM parquet.`examples/src/main/resources/users.parquet`")
```

### 保存模式

Save operations （保存操作）可以选择使用 `SaveMode` , 它指定如何处理现有数据如果存在的话. 重要的是要意识到, 这些 save modes （保存模式）不使用任何 locking （锁定）并且不是 atomic （原子）. 另外, 当执行 `Overwrite` 时, 数据将在新数据写出之前被删除.

| Scala/Java                        |    Any Language    | Meaning                                                      |
| --------------------------------- | :----------------: | ------------------------------------------------------------ |
| `SaveMode.ErrorIfExists`(default) | `"error"`(default) | 将 DataFrame 保存到 data source （数据源）时, 如果数据已经存在, 则会抛出异常. |
| `SaveMode.Append`                 |     `"append"`     | 将 DataFrame 保存到 data source （数据源）时, 如果 data/table 已存在, 则 DataFrame 的内容将被 append （附加）到现有数据中. |
| `SaveMode.Overwrite`              |   `"overwrite"`    | Overwrite mode （覆盖模式）意味着将 DataFrame 保存到 data source （数据源）时, 如果 data/table 已经存在, 则预期 DataFrame 的内容将 overwritten （覆盖）现有数据. |
| `SaveMode.Ignore`                 |     `"ignore"`     | Ignore mode （忽略模式）意味着当将 DataFrame 保存到 data source （数据源）时, 如果数据已经存在, 则保存操作预期不会保存 DataFrame 的内容, 并且不更改现有数据. 这与 SQL 中的 `CREATE TABLE IF NOT EXISTS` 类似. |

### 保存到持久表

`DataFrames` 也可以使用 `saveAsTable` 命令作为持久表保存到 Hive metastore 中。

现有的 Hive 部署不需要使用此功能。 Spark 将创建默认的 local Hive metastore（用 Derby ）与 `createOrReplaceTempView` 命令不同， `saveAsTable` 将实现DataFrame 的内容，并创建一个指向 Hive metastore 中数据的指针。即使 Spark 程序重新启动，持久性表仍然存在， 因为您保持与同一个 metastore 的连接. 可以通过使用表的名称在 `SparkSession` 上调用 `table` 方法来创建 persistent tabl （持久表）的 DataFrame .

对于基于文件的数据源，例如text，parquet，json等，可以通过`path`选项指定自定义表路径 ，例如`df.write.option("path", "/some/path").saveAsTable("t")`

删除表时，将不会删除自定义表路径，并且表数据仍然存在。如果未指定自定义表路径，则Spark会将数据写入 warehouse directory （仓库目录）。删除表时，也将删除默认表路径。

从 Spark 2.1 开始, persistent datasource tables （持久性数据源表）将 per-partition metadata （每个分区元数据）存储在 Hive metastore 中. 这带来了几个好处:

- 由于 metastore 只能返回查询的必要 partitions （分区）, 因此不再需要将第一个查询上的所有 partitions discovering 到表中.
- Hive DDLs 如 `ALTER TABLE PARTITION ... SET LOCATION` 现在可用于使用 Datasource API 创建的表.

请注意, 创建 external datasource tables （外部数据源表）（带有 `path` 选项）的表时, 默认情况下不会收集 partition information （分区信息）. **要 sync （同步） metastore 中的分区信息**, 可以调用 `MSCK REPAIR TABLE` .

### 分桶，分区和排序

<u>基于文件数据源</u>的可以进行分桶和排序输出到持久表

```scala
peopleDF.write.bucketBy(42, "name").sortBy("age").saveAsTable("people_bucketed")
```

在使用DataSet API时，`partitionBy`可以和 `save` and `saveAsTable`一起使用

```scala
usersDF.write.partitionBy("favorite_color").format("parquet").save("namesPartByColor.parquet")

//为单个表同时分区分桶
usersDF
  .write
  .partitionBy("favorite_color")
  .bucketBy(42, "name")
  .saveAsTable("users_partitioned_bucketed")
```



## Parquet 文件

[Parquet](http://parquet.io/)是一种柱状格式，许多其他数据处理系统都支持它。Spark SQL支持读取和写入Parquet文件，这些文件自动保留原始数据的模式。在编写Parquet文件时，出于兼容性原因，所有列都会自动转换为可为空。

### 以编程的方式加载数据

```scala
val peopleDF = spark.read.json("examples/src/main/resources/people.json")

// 保存parquet文件
peopleDF.write.parquet("people.parquet")

// 读取parquet文件
val parquetFileDF = spark.read.parquet("people.parquet")

// 创建临时表。然后进行SQL操作
parquetFileDF.createOrReplaceTempView("parquetFile")
val namesDF = spark.sql("SELECT name FROM parquetFile WHERE age BETWEEN 13 AND 19")
namesDF.map(attributes => "Name: " + attributes(0)).show()
```



### Partition Discovery（分区发现）

在分区表中，数据通常按分区存储在不同的目录中，每个分区目录中都有`partitioning column values encoded`（分区列值编码），Parquet文件可以自动发现和推断分区信息

```
path
└── to
    └── table
        ├── gender=male
        │   ├── ...
        │   │
        │   ├── country=US
        │   │   └── data.parquet
        │   ├── country=CN
        │   │   └── data.parquet
        │   └── ...
        └── gender=female
            ├── ...
            │
            ├── country=US
            │   └── data.parquet
            ├── country=CN
            │   └── data.parquet
            └── ...
```

通过将 path/to/table 传递给 SparkSession.read.parquet 或 SparkSession.read.load , Spark SQL 将自动从路径中提取 partitioning information （分区信息）,返回的 DataFrame 的 schema （模式）变成:

```scala
root
|-- name: string (nullable = true)
|-- age: long (nullable = true)
|-- gender: string (nullable = true)
|-- country: string (nullable = true)
```

从 Spark 1.6.0 开始, 默认情况下, partition discovery （分区发现）只能找到给定路径下的 partitions.对于上述示例, 如果用户直接使用`path/to/table/gender=male` 则 gender 将**不被**视为 partitioning column （分区列）.

用户可以指定 partition discovery 应该开始的基本路径, 则可以在数据源选项中设置 basePath.例如, 当 `path/to/table/gender=male`是数据的路径并且用户将 basePath 设置为 `path/to/table/`, gender 将是一个 



### 模式合并

用户可以从一个 simple schema （简单的架构）开始, 并根据需要逐渐向 schema 添加更多的 columns . 以这种方式, 用户可能会使用不同但相互兼容的 schemas 的 多个 Parquet 文件

1. 读取 Parquet 文件时, 将 data source option （数据源选项） mergeSchema 设置为 true
2. `spark.sql.parquet.mergeSchema=true`

```scala
Dataset<Row> squaresDF = spark.createDataFrame(squares, Square.class);
squaresDF.write().parquet("data/test_table/key=1");

Dataset<Row> cubesDF = spark.createDataFrame(cubes, Cube.class);
cubesDF.write().parquet("data/test_table/key=2");

Dataset<Row> mergedDF = spark.read().option("mergeSchema", true).parquet("data/test_table");//这里没有指定到key一级
mergedDF.printSchema();
// root
//  |-- value: int (nullable = true) //square和cube都有的值
//  |-- square: int (nullable = true)//square独有
//  |-- cube: int (nullable = true)//cube独有
//  |-- key: int (nullable = true)//分区值
```

### Hive metastore Parquet表协调

当读取和写入 Hive metastore Parquet 表时, Spark SQL 将尝试使用自己的 Parquet support （Parquet 支持）, 而不是 `Hive SerDe` ，来获得更好的性能

#### Hive / Parquet Schema Reconciliation

从 table schema processing （表格模式处理）的角度来说, Hive 和 Parquet 之间有两个关键的区别.

1. Hive 不区分大小写，而 Parquet 不是
2. Hive 认为所有列都可以为空，而 Parquet 中的可空性是非常重要的.

由于这个原因，当将 Hive metastore Parquet 表转换为 Spark SQL Parquet 表时，我们必须将Hive Metastore Schema 与 Parquet Schema 进行协调。协调规则是:

1. 在两个 schema 中具有相同名称的字段必须具有相同的数据类型，而不管 nullability（可空性）。协调字段应具有 Parquet 的数据类型，以遵循 nullability （可为空性）
2. reconciled schema （调和模式）正好包含 Hive metastore schema 中定义的那些字段.
   - 在 reconciled schema 中，仅出现在 Parquet schema 中的字段将被删除.
   - 在 reconciled schema 中，仅在 Hive metastore schema 中出现的字段将会添加为 nullable field （可空字段）。

#### 刷新元数据

Spark SQL缓存 Parquet 元数据以便获得更好的性能。启用Hive Metastore Parquet表转换后，这些转换后的表的元数据也会被缓存，如果这些表由Hive或者其他外部工具更新了，则需要手动刷新，以保持元数据一致。

```scala
//刷新元数据缓存
spark.catalog.refreshTable("my_table")
```

### 关于读写Parquet的配置

可以使用 `SparkSession` 上的 `setConf` 方法或使用 SQL 运行 `SET key = value` 命令来完成 Parquet 的配置.

| Property Name （参数名称）               | Default（默认） | Meaning（含义）                                              |
| ---------------------------------------- | --------------- | ------------------------------------------------------------ |
| `spark.sql.parquet.binaryAsString`       | false           | 一些其他 Parquet-producing systems （Parquet 生产系统）, 特别是 Impala, Hive 和旧版本的 Spark SQL , 在 writing out （写出） Parquet schema 时, 不区分 binary data （二进制数据）和 strings （字符串）. 该 flag 告诉 Spark SQL 将 binary data （二进制数据）解释为 string （字符串）以提供与这些系统的兼容性. |
| `spark.sql.parquet.int96AsTimestamp`     | true            | 一些 Parquet-producing systems , 特别是 Impala 和 Hive , 将 Timestamp 存入INT96 . 该 flag 告诉 Spark SQL 将 INT96 数据解析为 timestamp 以提供与这些系统的兼容性. |
| `spark.sql.parquet.cacheMetadata`        | true            | 打开 Parquet schema metadata 的缓存. 可以加快查询静态数据.   |
| `spark.sql.parquet.compression.codec`    | snappy          | 在编写 Parquet 文件时设置 compression codec （压缩编解码器）的使用. 可接受的值包括: uncompressed, snappy, gzip, lzo . |
| `spark.sql.parquet.filterPushdown`       | true            | 设置为 true 时启用 Parquet filter push-down optimization .   |
| `spark.sql.hive.convertMetastoreParquet` | true            | 当设置为 false 时, Spark SQL 将使用 Hive SerDe 作为 parquet tables , 而不是内置的支持. |
| `spark.sql.parquet.mergeSchema`          | false           | 当为 true 时, Parquet data source （Parquet 数据源） merges （合并）从所有 data files （数据文件）收集的 schemas , 否则如果没有可用的 summary file , 则从 summary file 或 random data file 中挑选 schema . |
| `spark.sql.optimizer.metadataOnly`       | true            | 如果为 true , 则启用使用表的 metadata 的 metadata-only query optimization 来生成 partition columns （分区列）而不是 table scans （表扫描）. 当 scanned （扫描）的所有 columns （列）都是 partition columns （分区列）并且 query （查询）具有满足 distinct semantics （不同语义）的 aggregate operator （聚合运算符）时, 它将适用. |